# Критические ошибки и способы их исправления

---

## 1. Data race в `BasicStateSaver` (критично)

**Статус:** уже исправлено в текущем коде, оставить здесь как напоминание о характере бага.

**Проблема:** Поля `BasicStateSaver` (`cpuTime`, `memTime`, `diskCTime`, `diskDTime`, `netCounterTime`) изменяются из разных горутин методами `SetCpuMetric`, `SetMemMetric`, `SetDiskCMetric`, `SetDiskDMetric`, `SetNetCountersMetric` без синхронизации. То же самое — чтение в `GetShutdownState` и в `GetLastShutdownState` (косвенно через заполнение полей из prevState). Это даёт data race при `go test -race`.

**Как исправить:**

- Добавить в структуру поле `mu sync.RWMutex`.
- В каждом методе `Set*Metric` брать `b.mu.Lock()`, в конце — `defer b.mu.Unlock()`.
- В `GetShutdownState` в начале взять `b.mu.RLock()`, в конце — `defer b.mu.RUnlock()`, и все чтения полей (`b.cpuTime`, `b.memTime` и т.д.) делать только внутри этой критической секции. Запись в поля при подстановке из `prevState` (если оставите эту логику) делать под `Lock()` в отдельном блоке или пересмотреть дизайн, чтобы не писать без лока.
- Убедиться, что нет других мест, где читаются/пишутся поля `BasicStateSaver` без того же мьютекса.

**Файл:** `worker/state/state.go`

---

## 2. Некорректное закрытие канала `workerJobs` и риск паники/дедлока

**Проблема:** В `Stop()` вызывается `defer close(bw.workerJobs)`. При этом основная горутина в `ExecuteJobs` может в этот момент выполнять `for j := range bw.workerJobs`. Закрытие канала из другой горутины при активном `range` приводит к корректному завершению цикла, но порядок событий у тебя смешан: сначала по `workerCtx.Done()` запускается горутина, которая вызывает `Stop()`, а цикл по `workerJobs` может ещё не закончиться. Кроме того, после `close(bw.workerJobs)` в канал больше нельзя отправлять — а `AppendToJobs` может ещё попытаться отправить job. В итоге возможны паника (запись в закрытый канал) или дедлок (блокировка при отправке/закрытии).

**Как исправить:**

- Определи единственного «владельца» канала `workerJobs`: та горутина, которая по нему делает `range` в `ExecuteJobs`. Только она должна закрывать `workerJobs` после того, как решит, что новых job’ов больше не будет.
- Не закрывать `workerJobs` в `Stop()`. В `Stop()` только сохраняй состояние (и при необходимости отменяй контекст). Закрытие канала делай в горутине `ExecuteJobs`: когда получила сигнал отмены (например, от контекста), перестань принимать новые job’ы (например, перестань читать из канала или закрой приём с другой стороны), дождись завершения уже запущенных job’ов (`wg.Wait()`), затем закрой `resCh` и выйди.
- Для новых мест, где может появиться запись в `workerJobs`, придерживаться той же схемы с `select { case <-ctx.Done(): return; default: bw.workerJobs <- job }`, как уже сделано в `AppendToJobs`.

**Файл:** `worker/worker.go`

---

## 3. Неверная проверка причины отмены в `MonitoringJob`

**Проблема:** В `case <-mjCtx.Done()` проверяется `ctx.Err() == context.DeadlineExceeded`. Родительский `ctx` может быть отменён по другой причине (например, Cancel), а таймаут достигнут у `mjCtx`. Логика сообщений («Stopped by Worker», «Finish monitoring job», «Stopping…») должна опираться на причину отмены того контекста, который реально используется в цикле — то есть `mjCtx.Err()`, а не `ctx.Err()`.

**Как исправить:**

- Проверять `mjCtx.Err()` вместо `ctx.Err()` для решений внутри этого `select`. Если нужна разница «отменили воркер (родитель)» vs «истекло время job’а» — сравнивать `mjCtx.Err()` с `context.DeadlineExceeded` и при необходимости отдельно проверять родительский `ctx.Err()` только для сообщения «Stopped by Worker».
- Аналогично проверить `job/waiting_job.go`: там та же схема с `jobCtx` и `ctx`.

**Файлы:** `job/monitoring_job.go`, `job/waiting_job.go`

---

## 4. Блокирующий вызов в HTTP‑обработчике

**Проблема:** В хендлере создания job’а вызывается `time.Sleep(10 * time.Second)`. Это блокирует горутину сервера на 10 секунд и ухудшает масштабируемость и отзывчивость сервера.

**Как исправить:**

- Удалить `time.Sleep(10 * time.Second)` из хендлера. Если задержка нужна для отладки или демо — вынести её в тест или делать асинхронно (например, запустить фоновую задачу с задержкой, а хендлер сразу вернуть ответ), но не блокировать обработчик запроса.

**Файл:** `server/handlers.go`

---

## 5. Интерфейс `Worker` не соответствует реализации

**Проблема:** В `worker/domain.go` интерфейс `Worker` объявляет `NewWorker(...) *Worker`, `AppendToJobs(job job.Job)` без контекста и `Stop()` без возврата ошибки. В реализации `BasicWorker`: конструктор — функция пакета `NewWorker`, `AppendToJobs(ctx context.Context, job job.Job)`, `Stop() error`. Интерфейс не совпадает с реализацией и не отражает контракт (конструктор в интерфейсе не нужен).

**Как исправить:**

- Убрать из интерфейса `Worker` метод `NewWorker` — конструкторы в интерфейсы в Go не входят.
- В интерфейсе объявить `AppendToJobs(ctx context.Context, job job.Job)` и `Stop() error`.
- Привести реализацию `BasicWorker` в соответствие с обновлённым интерфейсом (у тебя уже есть `ctx` и `error`, достаточно поправить только интерфейс).

**Файл:** `worker/domain.go`

---

## 6. Чтение состояния в `GetShutdownState` без блокировки

**Проблема:** В `GetShutdownState` читаются поля `b.cpuTime`, `b.memTime` и т.д. и при необходимости они перезаписываются из `prevState`. Эти поля одновременно пишутся из других горутин (Set*Metric). Без синхронизации это data race (даже если исправить пункт 1 только для Set*, чтение здесь тоже должно быть под RLock).

**Как исправить:**

- После введения `sync.RWMutex` в `BasicStateSaver` (п. 1) все чтения и записи полей в `GetShutdownState` выполнять под тем же мьютексом: для чтения — `RLock`/`RUnlock`, для записи (если оставляешь подстановку из prevState) — `Lock`/`Unlock` в отдельном блоке или переписать так, чтобы не писать без лока.

**Файл:** `worker/state/state.go`

---

## 7. Горутина, вызывающая `Stop()` по `workerCtx.Done()`, может отправить в уже закрытый `resCh`

**Проблема:** В `ExecuteJobs` горутина по `workerCtx.Done()` вызывает `bw.Stop()` и при ошибке делает `resCh <- job.Result{..., Error: err}`. К этому моменту основная горутина может уже закрыть `resCh` (например, после `wg.Wait()`). Отправка в закрытый канал вызывает панику.

**Как исправить:**

- Не отправлять результат ошибки `Stop()` в `resCh` из этой горутины. Либо логировать ошибку `Stop()`, либо передавать её другим способом (например, в отдельный канал ошибок с единственным потребителем, который обрабатывает его до закрытия `resCh`). Самое простое — не слать в `resCh` из горутины, слушающей `workerCtx.Done()`; закрывать `resCh` только после того, как эта горутина точно закончит работу и больше никто не будет писать в `resCh`.

**Файл:** `worker/worker.go`

---

## 8. Опечатка в сообщении пользователю

**Проблема:** В `job/waiting_job.go` в формате вывода указано `"veconds"` вместо `"seconds"`.

**Как исправить:** Заменить на `"seconds"` (или использовать `time.Duration` и `fmt` с правильным форматированием, например `diff.Seconds()`).

**Файл:** `job/waiting_job.go`

---

## 9. Запуск тестов с race detector

**Проблема:** Без запуска `go test -race ./...` критические гонки (п. 1, 7) могут не проявиться локально и попасть в прод.

**Как исправить:**

- Добавить в CI (и локально перед коммитом) запуск: `go test -race ./...`.
- Исправить все предупреждения race detector (в первую очередь после правок по п. 1 и 6).

---

Исправление пунктов 1, 2, 6 и 7 лучше делать согласованно в `worker/worker.go` и `worker/state/state.go`, так как они связаны с жизненным циклом горутин и каналов и с доступом к общему состоянию.
